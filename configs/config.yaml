run_mode: 'run_env' #train_evo / train_rl / run_env

general:
  seed: 4601

env:

  type: 'mine'
  name: 'TrackRunnerEnv'
  parameters:
    run_velocity: 0.015
    turn_degrees: 15
    track: '.\src\Envs\Tracks\tracky.pkl'
    max_steps: 250
    direction_randomization_coef: 0.1
#  name: 'MountainCarEnv'
#  parameters:
#    mass: 1
#    friction_coef: 0.001
#    thrust: 5
#    max_steps: 500
#  name: 'GridWorldEnv'
#  parameters:
#    rows: 4
#    cols: 4
#    max_steps: 200
#  type: 'gym'
#  name: CartPole-v0 # FrozenLake-v1 / CartPole-v0



train_rl:
  rl_algorithms:
    AlgorithmDQN:
      reward_discount: 0.99
      model_learning_rate: 0.001
      target_update_interval: 0 # Set 0 to disable target model and use the same model for target and current q
      epsilon_parameters:
        start: 0.0
        end: 0.000
        decay: 200
      model_config:
        hidden_layers_dims: [ 1024,512 ]
        save_file: ~
  #    AlgorithmActorCritic:
  #      reward_discount: 0.95
  #      model_learning_rate: 0.00005
  #      model_config:
  #        hidden_layers_dims: [ 1024,512 ]
  #        save_file: ~

  trainer_parameters:
    batch_size: 256
    max_episodes: 20000
    test_every: 100
    test_episodes: 10
    output_dir_path: '.\output\rl_agents'
    experience_memory: True # True to sample uniformly from memory, False to get only the last 'batch_size' items

train_evo:
  specimen_count: 200
  survivor_count: 20
  max_iterations: 20
  mutation_rate: 0.0001
  generation_method: "Random Splice"
  fitness_target: -50
  output_dir: '.\output\evo_agents'

run_env:
  #  agent_weights_file_path: .\output\rl_agents\TrackRunnerEnv_2022_06_25-23_09\agent_parameters__episode_1750___fitness_1_00.pkl
  agent_weights_file_path: F:\Study\Programming\PycharmProjects\Reinforcement-Learning\output\rl_agents\TrackRunnerEnv_2022_07_18-11_42\agent_parameters__episode_300___fitness_0_68.pkl
  verbose: False
  frame_rate: 60
  runs: 10